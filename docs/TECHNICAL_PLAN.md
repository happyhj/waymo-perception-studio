# Technical Plan â€” Waymo Perception Studio

Portfolio project targeting Waymo Fullstack Engineer (Applications & Tools) role.
Timeline: 4â€“5 days.

## 1. Architecture Decisions

### Data Pipeline â€” Browser-Native Parquet (Zero Preprocessing)

**Decision**: Read Parquet files directly in the browser via `hyparquet` (pure JS). No Python, no server, no preprocessing.

**Why this works**: Parquet files have footer metadata with row group offsets. Browser reads footer (few KB) first, then fetches only the needed row group via `File.slice(offset, length)`. This enables random access into 328MB camera_image and 162MB lidar files without loading them fully into memory.

**Why this matters**: v1.0 TFRecord requires sequential reads via TensorFlow â€” that's why erksch needed a Python server. v2.0 Parquet was designed for selective/columnar access, and we exploit this to the maximum: zero-install, browser-only, file-in â†’ visualization-out. This should be emphasized in interviews.

### Data Loading (Two Modes)

- **Local dev**: `VITE_WAYMO_DATA_PATH=./waymo_data` in `.env`. Vite serves data as static assets. Auto-loads on startup.
- **Deployed demo / visitors**: Folder drag & drop into browser. App scans `{component}/{segment_id}.parquet` structure, auto-detects segments/components. If multiple segments, show picker.
- 3DGS Lab tab always works immediately (bundled .ply).

### Licensing

- **Raw data**: Cannot redistribute. Users must download from Waymo (free, license agreement required).
- **Trained model weights (.ply)**: Distributable (non-commercial). Pre-built 3DGS .ply bundled with app.
- Result: 3DGS Lab = zero-download demo. Sensor View = user provides data.

## 2. Waymo v2.0 Data Structure

Files: `{component}/{segment_id}.parquet`
Key columns: `key.segment_context_name`, `key.frame_timestamp_micros`, `key.laser_name` (1-5), `key.camera_name` (1-5)

### Sample Segment: `10023947602400723454_1120_000_1140_000`
- SF downtown, daytime, sunny
- 199 frames (~20 sec at 10Hz)
- Avg 94 objects/frame, 115 unique tracked objects
- Types: 1=VEHICLE(36/frame), 2=PEDESTRIAN(33), 3=SIGN(23), 4=CYCLIST(1)

### LiDAR â€” CRITICAL: Range Image format, NOT xyz points

`lidar` component stores **range images**, not point clouds. Must convert to xyz in browser.

| LiDAR | laser_name | Range Image Shape | Pixels |
|-------|-----------|-------------------|--------|
| TOP | 1 | 64 Ã— 2650 Ã— 4 | 169,600 |
| FRONT | 2 | 200 Ã— 600 Ã— 4 | 120,000 |
| SIDE_LEFT | 3 | 200 Ã— 600 Ã— 4 | 120,000 |
| SIDE_RIGHT | 4 | 200 Ã— 600 Ã— 4 | 120,000 |
| REAR | 5 | 200 Ã— 600 Ã— 4 | 120,000 |

- 4 channels = [range, intensity, elongation, is_in_no_label_zone]
- Total ~649K range pixels/frame (valid points fewer â€” filter range > 0)
- Two returns per pulse: `range_image_return1` (primary) and `range_image_return2` (secondary reflection). MVP uses return1 only.

#### Range Image â†’ XYZ Conversion Math

Source: Official Waymo SDK `lidar_utils.convert_range_image_to_point_cloud()` and GitHub issues #656, #51, #307, #863.

**Step 1: Compute inclination and azimuth per pixel**

- **Inclination** (vertical angle):
  - TOP LiDAR: **non-uniform** â€” `lidar_calibration` provides a `beam_inclination.values` array (64 exact angles, one per row).
  - Other 4 LiDARs: **uniform** â€” only `beam_inclination.min` and `beam_inclination.max` provided. Linear interpolation: `inclination = max - (row / height) * (max - min)` (row 0 = top = max angle).
- **Azimuth** (horizontal angle):
  - `azimuth = azimuth_offset + (col / width) * 2Ï€`
  - Column 0 = rear direction (azimuth â‰ˆ Ï€), center column = forward (azimuth â‰ˆ 0).

**Step 2: Spherical â†’ Cartesian**

```
x = range Ã— cos(inclination) Ã— cos(azimuth)
y = range Ã— cos(inclination) Ã— sin(azimuth)
z = range Ã— sin(inclination)
```

Skip pixels where `range <= 0` (invalid).

**Step 3: Extrinsic transform (sensor frame â†’ vehicle frame)**

Apply 4Ã—4 extrinsic matrix from `lidar_calibration`:
```
[x_v, y_v, z_v, 1]áµ€ = extrinsic Ã— [x, y, z, 1]áµ€
```

**Step 4: Per-point ego-motion correction (TOP only)**

`lidar_pose` provides a per-pixel vehicle pose for the TOP LiDAR to correct rolling shutter distortion. Other 4 LiDARs don't need this (their sweep is fast enough). For MVP, this step can be deferred â€” the visual difference is subtle.

#### Conversion Strategy: WebGPU Compute Shader + CPU Web Worker Fallback

The conversion is **embarrassingly parallel** â€” each pixel is independent (cos, sin, matrix mul). Two implementations:

- **WebGPU Compute Shader** (primary): 649K pixels as GPU threads. Expected ~1-2ms/frame. Available in Chrome, Edge, Safari 17.4+.
- **CPU Web Worker** (fallback): For Firefox and older browsers. Expected ~30-50ms/frame.

Both paths share the same math; only the execution environment differs. Benchmark comparison in README demonstrates the speedup.

```
src/utils/rangeImage.ts        â† Pure conversion math (shared, testable)
src/workers/lidarWorker.ts     â† CPU fallback (Web Worker)
src/utils/rangeImageGpu.ts     â† WebGPU compute shader
src/hooks/useLidarConverter.ts â† Auto-selects GPU or Worker
```

#### Gotchas from Waymo SDK Issues

- **#656**: beam_inclination.values can be null for uniform sensors â€” always check before using.
- **#307**: Raw data is already corrected to vehicle frame â€” don't apply additional azimuth corrections.
- **#863**: When merging DataFrames, laser_name must match between lidar and lidar_calibration.
- **#51**: range_image_top_pose is per-pixel, not per-frame â€” only TOP LiDAR has this.

#### Reference: erksch viewer (v1.0)

erksch doesn't do this conversion in the browser at all. Python server calls `frame_utils.convert_range_image_to_point_cloud()` (official Waymo util with TensorFlow), converts to xyz, then sends `[x, y, z, intensity, laser_id, label]` as Float32 binary over WebSocket. Our project does this **entirely in the browser** â€” no Python, no TensorFlow.

### Camera

| Camera | camera_name | Resolution |
|--------|-----------|------------|
| FRONT | 1 | 1920 Ã— 1280 |
| FRONT_LEFT | 2 | 1920 Ã— 1280 |
| FRONT_RIGHT | 3 | 1920 Ã— 1280 |
| SIDE_LEFT | 4 | 1920 Ã— 886 |
| SIDE_RIGHT | 5 | 1920 Ã— 886 |

- `camera_image` stores JPEG binary in `[CameraImageComponent].image`
- `camera_segmentation` is **1Hz** (not 10Hz) â€” only 20 frames have segmentation

### File Sizes (1 segment = ~597MB total)

| Component | Size | Load Strategy |
|-----------|------|--------------|
| camera_image | 328MB | Lazy per-frame (row group) |
| lidar | 162MB | Lazy per-frame (row group, 4 RGs) |
| lidar_camera_projection | 74MB | Lazy per-frame |
| lidar_pose | 22MB | Lazy per-frame |
| camera_segmentation | 2.3MB | Load at startup |
| lidar_box | 976KB | Load at startup |
| lidar_camera_synced_box | 543KB | Load at startup |
| lidar_segmentation | 531KB | Load at startup |
| projected_lidar_box | 611KB | Load at startup |
| camera_box | 291KB | Load at startup |
| camera_hkp | 116KB | Load at startup |
| lidar_hkp | 29KB | Load at startup |
| vehicle_pose | 28KB | Load at startup |
| stats | 24KB | Load at startup |
| camera_to_lidar_box_association | 24KB | Load at startup |
| camera_calibration | 8.8KB | Load at startup |
| lidar_calibration | 4.7KB | Load at startup |

### Component Schemas (Key Columns)

**lidar_box** (18,633 rows = ~94/frame Ã— 199 frames):
- `key.laser_object_id` â€” tracking ID, persistent across frames
- `[LiDARBoxComponent].box.center.{x,y,z}` â€” double
- `[LiDARBoxComponent].box.size.{x,y,z}` â€” double
- `[LiDARBoxComponent].box.heading` â€” double
- `[LiDARBoxComponent].type` â€” int8 (1=vehicle, 2=pedestrian, 3=sign, 4=cyclist)
- `[LiDARBoxComponent].speed.{x,y,z}` â€” double
- `[LiDARBoxComponent].acceleration.{x,y,z}` â€” double

**vehicle_pose** (199 rows):
- `[VehiclePoseComponent].world_from_vehicle.transform` â€” fixed_size_list<double>[16] (4Ã—4 matrix)

**lidar_calibration** (5 rows):
- `[LiDARCalibrationComponent].extrinsic.transform` â€” fixed_size_list<double>[16]
- `[LiDARCalibrationComponent].beam_inclination.{min,max}` â€” double
- `[LiDARCalibrationComponent].beam_inclination.values` â€” list<double>

**camera_calibration** (5 rows):
- `[CameraCalibrationComponent].intrinsic.{f_u,f_v,c_u,c_v,k1,k2,p1,p2,k3}` â€” double
- `[CameraCalibrationComponent].extrinsic.transform` â€” fixed_size_list<double>[16]
- `[CameraCalibrationComponent].{width,height}` â€” int32

**camera_image** (995 rows = 5 cameras Ã— 199 frames):
- `[CameraImageComponent].image` â€” binary (JPEG)
- `[CameraImageComponent].pose.transform` â€” fixed_size_list<double>[16]

**lidar** (995 rows = 5 LiDARs Ã— 199 frames):
- `[LiDARComponent].range_image_return1.values` â€” list<float>
- `[LiDARComponent].range_image_return1.shape` â€” fixed_size_list<int32>[3]
- `[LiDARComponent].range_image_return2.{values,shape}` â€” second return

## 3. Reference Projects (Prior Art)

### erksch/waymo-open-dataset-viewer (2019)
- **GitHub**: https://github.com/erksch/waymo-open-dataset-viewer
- **Stack**: Webpack + TypeScript, Python WebSocket server (TensorFlow GPU)
- **What it does**: LiDAR point cloud (5 sensors), 3D bounding boxes, point color by label/intensity, per-LiDAR toggle, frame slider, OrbitControls
- **What it doesn't do**: No camera images, no map data, no segmentation, no play/pause animation (commented out)
- **Architecture**: Python server reads v1.0 TFRecord â†’ parses via TensorFlow â†’ streams binary frames over WebSocket â†’ Three.js renders in browser
- **Key limitation**: v1.0 only, TensorFlow dependency, requires Python server
- **What we learn**: Basic Three.js point cloud rendering approach (BufferGeometry + Points), WebSocket frame streaming pattern, UI layout inspiration

### Foxglove Studio (industry standard)
- **GitHub (open-source fork)**: https://github.com/AD-EYE/foxglove-opensource (v1.87.0, MPL-2.0)
- **Stack**: React + Three.js, desktop app (Electron)
- **What it does**: Multi-panel layout (diagnostics, aerial view, camera views, 3D perspective), camera segmentation overlay, play/pause/speed control, dual 3D views, camera vs LiDAR object count comparison
- **What it doesn't do**: No Waymo v2.0 Parquet support (requires ROS/MCAP conversion), no 3DGS
- **Key insight**: Panel-based "studio" UI pattern â€” resizable panels, multiple views of same data. Our UI layout is inspired by this.
- **Closed source since v2.0**: Only open-source fork (v1.87.0) is available

### hailanyi/3D-Detection-Tracking-Viewer (522 stars)
- **GitHub**: https://github.com/hailanyi/3D-Detection-Tracking-Viewer
- **Stack**: Python + VTK/Vedo, desktop app
- **What it does**: GT vs Prediction dual box rendering (red/blue), tracking ID color mapping via matplotlib colormaps, 3D car OBJ model rendering, 2D camera projection, supports KITTI + Waymo (OpenPCDet format)
- **What it doesn't do**: No browser, requires Python + preprocessed npy/pkl files, no 3DGS
- **What we adopt**: (1) Tracking ID â†’ rainbow colormap per `laser_object_id`, (2) BoxType-specific 3D meshes (car/pedestrian/cyclist), (3) Camera frustum visualization in 3D view
- **What we skip**: GT vs Prediction comparison (not portfolio-relevant for a viewer tool)

### Street Gaussians (ECCV 2024)
- **GitHub**: https://github.com/zju3dv/street_gaussians
- **Paper**: Street Gaussians for Modeling Dynamic Urban Scenes
- **What it does**: Static background (standard 3DGS) + dynamic foreground (tracked pose + 4D SH). Clean background rendering with vehicle removal. Novel view synthesis including BEV.
- **Performance**: PSNR ~28, 135 FPS, ~30 min training per segment
- **Why we use it**: Waymo has no top-down camera â†’ need Novel View Synthesis for BEV â†’ 3DGS is the approach. Street Gaussians is Waymo-compatible and fast to train.

### gsplat.js
- **GitHub**: https://github.com/dylanebert/gsplat.js
- **What it does**: Browser-native Gaussian Splat rendering. Loads .ply files, renders in WebGL.
- **Why we use it**: Renders Street Gaussians .ply output directly in browser. Separate canvas from R3F Three.js to avoid WebGL context conflicts.

## 4. Differentiation Matrix

| Feature | erksch (2019) | Foxglove (2024) | Ours (2026) |
|---------|--------------|----------------|-------------|
| Dataset | v1.0 TFRecord | ROS/MCAP | **v2.0 Parquet native** |
| Server | Python + TF | Desktop App | **None (browser)** |
| LiDAR | âœ… | âœ… | âœ… |
| Camera | âŒ | âœ… | âœ… |
| Segmentation | âŒ | âœ… | âœ… |
| Dual 3D View | âŒ | âœ… | âœ… |
| 3DGS BEV | âŒ | âŒ | **âœ… Killer Feature** |

## 4. Visualization Features

- **Tracking ID color mapping**: `key.laser_object_id` persistent across frames. Rainbow colormap per ID.
- **3D vehicle OBJ models**: BoxType-specific meshes (car, pedestrian, cyclist) instead of wireframe boxes.
- **Camera frustum visualization**: Semi-transparent frustums in 3D view showing each camera's FOV.

## 5. UI Design

Dark theme (#1a1a2e). Two tabs: [Sensor View] [3DGS Lab ğŸ§ª]

### Sensor View
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Camera Views (5)       â”‚ Bird's Eye View         â”‚
â”‚ FL | F | FR            â”‚ (3DGS or LiDAR BEV)     â”‚
â”‚ SL |   | SR            â”‚                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3D LiDAR View (point cloud + bbox, OrbitControls)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â—€ â–¶  â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 00:05/00:20   1x         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Loading Screen
- "Open Waymo Dataset Folder" button + folder drag & drop
- After load: component checklist (lidar âœ… camera âœ… segmentation âŒ etc.)

### 3DGS Lab
- Full viewport gsplat.js renderer with pre-built .ply
- Separate tab to avoid WebGL context conflicts

## 6. Implementation Phases

1. **MVP (2 days)**: Parquet loading + LiDAR point cloud (range imageâ†’xyz) + bounding boxes + timeline
2. **Camera (1 day)**: 5 camera panels + Camera-LiDAR sync + segmentation overlay
3. **Dual View (0.5 day)**: Aerial + Perspective split
4. **3DGS BEV (1 day)**: Street Gaussians training + .ply export + gsplat.js renderer
5. **Polish (0.5 day)**: README, deployment, demo GIF, LinkedIn post

## 7. 3DGS Strategy

### Approach: Street Gaussians (ECCV 2024)
- Static background (standard 3DGS) + Dynamic foreground (tracked pose + 4D SH)
- ~30 min training per segment, 135 FPS rendering
- Export .ply â†’ bundle with app â†’ orthographic BEV camera

### Distribution
- Trained .ply is distributable under Waymo license (non-commercial)
- Same segment as README's recommended download â†’ direct raw-vs-reconstructed comparison
- 3DGS Lab works with zero data download

## 8. Performance Notes

- LiDAR range image â†’ xyz: WebGPU Compute Shader (~1-2ms) with CPU Web Worker fallback (~30-50ms)
- Point cloud: BufferGeometry + Points
- Bounding boxes: InstancedMesh (avg 94/frame)
- Lazy frame loading: current Â± N frames in memory, prefetch ahead
- camera_image/lidar: row-group random access, never full file
- Calibrations + boxes + poses: full load at startup (<2MB total)
- Perf-critical rendering: useFrame + imperative refs
- WebGPU availability: Chrome 113+, Edge 113+, Safari 17.4+. Firefox fallback to Web Worker.

### R3F vs Vanilla Three.js â€” ì„±ëŠ¥ ë™ë“±ì„±

R3F(@react-three/fiber)ëŠ” Three.js ìœ„ì˜ ì–‡ì€ React ë°”ì¸ë”©ì´ë©°, ë Œë” ë£¨í”„ ìì²´ëŠ” ë™ì¼í•œ Three.js `WebGLRenderer`ê°€ ì‹¤í–‰í•œë‹¤. ì„±ëŠ¥ ì°¨ì´ê°€ ì—†ëŠ” ì´ìœ :

1. **ë Œë” ë£¨í”„**: R3Fì˜ `useFrame` í›…ì€ Three.jsì˜ `requestAnimationFrame` ë£¨í”„ì— ì§ì ‘ ì½œë°±ì„ ë“±ë¡. í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì—…ë°ì´íŠ¸ ì‹œ `bufferGeometry.attributes.position.needsUpdate = true`ë¥¼ imperativeí•˜ê²Œ í˜¸ì¶œ â€” vanilla Three.jsì™€ ì™„ì „íˆ ë™ì¼í•œ ì½”ë“œ ê²½ë¡œ.

2. **Draw call ìµœì†Œí™”**: 168K í¬ì¸íŠ¸ë¥¼ ê°œë³„ `<mesh>`ë¡œ ë§Œë“¤ë©´ React reconciliation ì˜¤ë²„í—¤ë“œ ë°œìƒ. ìš°ë¦¬ëŠ” `BufferGeometry` + `<points>`ë¡œ **í•œ ë²ˆì˜ draw call**ì— ì „ì²´ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë Œë”. ë°”ìš´ë”© ë°•ìŠ¤ë„ `InstancedMesh`ë¡œ 94ê°œë¥¼ **1 draw call**.

3. **React ì˜¤ë²„í—¤ë“œ êµ¬ê°„**: ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸/ì–¸ë§ˆìš´íŠ¸ ì‹œ Three.js ì˜¤ë¸Œì íŠ¸ ìƒì„±/ì‚­ì œì—ë§Œ ë°œìƒ. ì´ê±´ ì´ˆê¸°í™” ë•Œ í•œ ë²ˆì´ê³  60fps ë Œë” ë£¨í”„ì—ëŠ” ì˜í–¥ ì—†ìŒ.

4. **R3F ì„ íƒ ì´ìœ **: Waymo JDê°€ React/TypeScriptë¥¼ ëª…ì‹œ. R3FëŠ” React ìƒíƒœê³„ ìˆ™ë ¨ë„ë¥¼ ë³´ì—¬ì£¼ë©´ì„œë„ Three.js ì„±ëŠ¥ì„ ê·¸ëŒ€ë¡œ ìœ ì§€. ì„ ì–¸ì  ì”¬ ê·¸ë˜í”„ êµ¬ì„± (ì¹´ë©”ë¼ íŒ¨ë„, ì»¨íŠ¸ë¡¤ ë“±)ì—ì„œ ê°œë°œ ìƒì‚°ì„±ë„ ë†’ìŒ.

## 9. Interview Narrative

"I analyzed Foxglove Studio and erksch's viewer, then built a Waymo v2.0-native visualization tool. Existing viewers require Python + TensorFlow servers or ROS conversion. Mine reads v2.0 Parquet natively in the browser â€” no server, no install. The 3DGS Bird's Eye View is unique â€” Waymo has no top-down camera, so I used Street Gaussians for Novel View Synthesis. I applied WebGL optimization experience from View360 (530+ stars, adopted by Amazon.com)."

## 10. Decision Log

Chronological record of technical decisions and the reasoning behind them.

### D1. Project Name â†’ `waymo-perception-studio`
- **Alternatives considered**: waymo-viewer, waymo-3d-viewer, waymo-scene-studio, wod-viewer
- **Reasoning**: "Waymo" gives brand recognition on LinkedIn/GitHub. "Perception" matches the Waymo Perception team and dataset domain. "Studio" implies multi-panel tool (not just a simple viewer) â€” justified by our multi-view layout, tab system, and panel customization. Foxglove also calls itself "Studio."
- **Rejected**: `wod-viewer` (too obscure â€” only paper authors know "WOD"), `waymo-scene-studio` (less domain-specific)

### D2. Build Tool â†’ Vite over Webpack/Next.js
- **Alternatives considered**: Webpack (erksch used it), Next.js, Vite
- **Reasoning**: (1) Near-instant dev server startup via native ES modules â€” critical for iterating on Three.js scenes. Webpack bundles everything upfront. (2) 2026 standard for React + TS SPA â€” CRA is deprecated, Next.js is overkill for pure client-side app (no SSR needed). (3) Web Worker support out of the box (`new Worker(new URL(..., import.meta.url))`). (4) `waymo_data/` static serving trivial via config. (5) erksch used Webpack in 2019 â€” Vite modernizes the stack.
- **Rejected**: Next.js (SSR/SSG unnecessary, adds complexity for what is a pure browser-side tool)

### D3. 3D Library â†’ @react-three/fiber (R3F) over Vanilla Three.js
- **Reasoning**: Waymo JD emphasizes React/TypeScript. R3F demonstrates React ecosystem mastery. Declarative patterns for UI-heavy panels (cameras, controls). Performance-critical paths (200K point cloud) handled via `useFrame` + imperative refs â€” no performance gap vs vanilla.
- **Trade-off accepted**: Slightly larger bundle, but developer velocity and interview alignment win.

### D4. Data Pipeline â†’ Browser-native Parquet, no Python preprocessing
- **Alternatives considered**: (A) Python script â†’ JSON/Binary, (B) Browser Parquet parsing
- **Reasoning**: Option B makes "Parquet native" claim genuine. Zero setup friction vs erksch (Python + TF). Leverages Parquet row-group random access â€” read footer metadata, then slice into specific frames without loading full file. This is only possible because Waymo v2.0 chose Parquet over v1.0 TFRecord (sequential-only).
- **Key insight**: `File.slice(offset, length)` in browser = HTTP Range Request for local files. Footer metadata gives row group offsets. 328MB camera_image becomes manageable.

### D5. No GitHub Pages for data â†’ Local-first with deployed demo for 3DGS
- **Reasoning**: Waymo license prohibits data redistribution. erksch also requires "download yourself." But we go further: deployed URL hosts the app + bundled 3DGS .ply (trained weights are distributable). Sensor View requires user data; 3DGS Lab works immediately.
- **User flow impact**: Interview â†’ share URL â†’ 3DGS BEV loads instantly â†’ "wow" moment â†’ motivated to try Sensor View.

### D6. Data loading â†’ .env path (dev) + folder drag & drop (deployed)
- **Alternatives considered**: (A) Drag & drop only, (B) showDirectoryPicker() only, (C) .env + dev server
- **Reasoning**: Developers clone â†’ .env path â†’ auto-load (zero friction). Visitors â†’ drag & drop `waymo_data/` folder. `showDirectoryPicker()` as bonus for Chrome/Edge. Multiple entry points, same parsing pipeline.
- **Rejected**: Individual file drag & drop (17 component folders with same filename â€” would overwrite each other).

### D7. 3DGS .ply is distributable â†’ bundle with app
- **Evidence**: Waymo license explicitly allows "trained model architectures, including weights and biases, developed using the Dataset" for non-commercial purposes.
- **Impact**: Killer feature (3DGS BEV) becomes zero-download demo. Same segment as recommended gsutil download â†’ raw vs reconstructed comparison possible.

### D8. LiDAR data is range images â†’ need spherical-to-cartesian conversion
- **Discovery**: Parquet analysis revealed `lidar` component stores range images (64Ã—2650Ã—4 for TOP, 200Ã—600Ã—4 for others), not xyz point clouds.
- **Impact**: Must implement range image â†’ xyz conversion in browser. Uses beam inclination angles from `lidar_calibration` + extrinsic matrix. CPU intensive â†’ Web Worker.
- **Positive spin**: This is non-trivial engineering that demonstrates understanding of LiDAR data representation â€” good interview talking point.

### D9. Tracking ID â†’ zero-cost tracking visualization
- **Discovery**: `lidar_box` has `key.laser_object_id` that persists across frames for the same physical object. 115 unique objects in sample segment.
- **Impact**: Assign color per object ID with rainbow colormap â†’ automatic tracking visualization with no additional ML. Same color = same car across 20 seconds.

### D10. Camera segmentation is 1Hz, not 10Hz
- **Discovery**: `camera_segmentation` has 100 rows (5 cameras Ã— 20 frames), not 995. Segmentation is sampled at 1Hz.
- **Impact**: Segmentation overlay only updates every 10 frames. UI should indicate when segmentation data is available vs interpolated/unavailable.

### D11. Parquet row-group structure enables lazy loading without preprocessing
- **Discovery**: lidar file has 4 row groups (~50 frames each). camera_image also has 4 row groups. Browser can read specific row groups without loading the entire file.
- **Impact**: No preprocessing step needed. User drags folder â†’ app reads Parquet footer â†’ fetches frame data on demand. This is the fundamental architectural advantage over v1.0 TFRecord approach.
- **Interview angle**: "I chose v2.0 Parquet specifically because its columnar format enables browser-native random access â€” something impossible with v1.0 TFRecord."

### D12. Lazy loading mechanism â€” File.slice() vs HTTP Range Requests
- **Two paths, same interface**: hyparquet's `AsyncBuffer` abstracts byte access. We implement two backends:
  - **Drag & drop (File API)**: `file.slice(start, end).arrayBuffer()` â€” reads bytes from local file handle, no network involved.
  - **Static serving (Vite dev / deployed)**: `fetch(url, { headers: { Range: 'bytes=start-end' } })` â†’ server responds `206 Partial Content` with only the requested bytes. Vite dev server, nginx, S3, Cloudflare Pages all support Range Requests by default.
- **Loading flow**: (1) Read last 8 bytes â†’ get footer length. (2) Read footer (few KB) â†’ get all row group offsets/sizes. (3) On frame request â†’ read only that row group's byte range â†’ decode.
- **Result**: 328MB camera_image file, but each frame request reads ~1.6MB. No server-side logic. No preprocessing.
- **Why this works**: Parquet was designed for distributed storage (HDFS, S3) where Range Requests are the access primitive. We're just using the same mechanism in the browser.
- **AsyncBuffer interface**:
  ```typescript
  interface AsyncBuffer {
    byteLength: number
    slice(start: number, end: number): Promise<ArrayBuffer>
  }
  // File API backend
  const fileBuffer: AsyncBuffer = {
    byteLength: file.size,
    slice: (s, e) => file.slice(s, e).arrayBuffer()
  }
  // HTTP Range Request backend
  const urlBuffer: AsyncBuffer = {
    byteLength: totalSize,
    slice: (s, e) => fetch(url, {
      headers: { Range: `bytes=${s}-${e - 1}` }
    }).then(r => r.arrayBuffer())
  }
  ```

### D13. Component merge strategy â€” JS port of Waymo's v2.merge()
- **Source**: Official Waymo Python SDK `v2.dataframe_utils.merge()` ([GitHub](https://github.com/waymo-research/waymo-open-dataset/blob/master/src/waymo_open_dataset/v2/dataframe_utils.py))
- **What the official API does**: (1) Auto-detect join keys via `key.` prefix. (2) Find common keys between two tables (intersection). (3) Optional `groupby â†’ agg(list)` to prevent cartesian product when row counts differ. (4) Pandas merge on common keys.
- **Our JS port** (`src/utils/merge.ts`): Same logic, but using `Map` lookup instead of Pandas merge. Faster in browser (no Pandas overhead).
- **Key insight from reference projects**:
  - erksch (v1.0): No join needed â€” TFRecord bundles everything per frame.
  - 3D-Detection-Tracking-Viewer: Pre-processes into per-frame `.npy` files â€” join happens offline.
  - Waymo official v2.0: `key.frame_timestamp_micros` is the universal join key. Components with different granularity (per-frame vs per-sensor vs per-object) joined via common key intersection.
- **Our approach**: Port v2.merge() to TypeScript. `vehicle_pose` (199 rows) provides master frame list. All other components join via `key.frame_timestamp_micros` + optional `key.camera_name` / `key.laser_name`.
- **Interview angle**: "I studied the official Waymo v2 Python SDK's merge strategy and ported it to TypeScript for browser-native use â€” same relational join pattern, zero Python dependency."

### D15. Real Data Observations â€” Range Image Conversion

ì‹¤ì œ Waymo v2.0 ë°ì´í„°ë¡œ range image â†’ xyz ë³€í™˜ì„ êµ¬í˜„í•˜ë©´ì„œ ë°œê²¬í•œ ì‚¬ì‹¤ë“¤:

**Range image format**:
- ValuesëŠ” `number[]` (not Float32Array). hyparquetê°€ Parquetì˜ float listë¥¼ JS Arrayë¡œ ë””ì½”ë”©í•¨.
- 4 channels [range, intensity, elongation, nlz] ì´ flat arrayë¡œ ì¸í„°ë¦¬ë¸Œë¨: `[r0, i0, e0, n0, r1, i1, e1, n1, ...]`
- Shapeì€ `[height, width, channels]` = `[64, 2650, 4]` for TOP â†’ 169,600 pixels Ã— 4 values = 678,400 elements.

**Valid pixel ratio**:
- TOP lidar: 149,796 valid (range > 0) out of 169,600 total â†’ **88.3%** valid.
- Invalid pixels have `range = -1` (not `0`). Filter condition `range > 0` correctly handles both.
- First ~32 pixels in row 0 are typically invalid (very top of FOV, sky region).

**Spatial distribution (vehicle frame)**:
- Range: 2.3m ~ 75m (this segment, SF downtown).
- X/Y: max distance ~75m from vehicle center, reasonable for urban lidar.
- **Z range: -20m ~ +30m** â€” much wider than the naÃ¯ve expectation of "ground at -2m, buildings at +10m". SF downtown has steep hills, underground parking ramp exits visible to lidar, and tall buildings/overpasses. Test thresholds adjusted to `[-25, +40]`.

**Beam inclination**:
- TOP (`laser_name=1`): `beam_inclination.values` is a 64-element `number[]` â€” non-uniform spacing, denser near horizon.
- FRONT/SIDE/REAR (`laser_name=2-5`): `beam_inclination.values` is `undefined` (not present in Parquet row). Must use `min`/`max` for uniform linear interpolation. Height is 200 rows for these sensors.

**Extrinsic calibration**:
- All 5 sensors have 16-element `number[]` (4Ã—4 row-major). TOP's extrinsic includes ~1.8m Z translation (roof mount height).
- Extrinsic transforms sensor-frame xyz â†’ vehicle-frame xyz. Vehicle frame: X=forward, Y=left, Z=up.

**Performance (CPU, single thread, M2 MacBook Air)**:
- TOP (64Ã—2650, 149K valid points): ~2.3ms per frame
- All 5 sensors merged: ~5ms per frame, ~168K total points
- Already fast enough for 10Hz (100ms budget). WebGPU will further improve in real browser with hardware GPU.
- Dawn software renderer (Node.js `webgpu` pkg): ~35ms â€” slower due to no hardware acceleration. Browser Metal/Vulkan path expected ~1-2ms.

### D14. Waymo Parquet uses BROTLI compression â†’ hyparquet-compressors required
- **Discovery**: All Waymo v2.0 Parquet files use BROTLI compression codec. hyparquet core only includes Snappy; BROTLI requires `hyparquet-compressors` plugin.
- **Why BROTLI**: Standard in Google's big data stack (BigQuery, Cloud Storage). ~20-30% better compression than GZIP on structured data. Waymo chose it for storage efficiency across petabyte-scale datasets.
- **Why this is good for us**: BROTLI was originally designed by Google for web content delivery (`Content-Encoding: br`). Browser-native support exists for HTTP streams. The JS WASM decompressor in hyparquet-compressors is well-optimized. So Waymo's infrastructure choice accidentally aligns perfectly with browser-based access.
- **Impact**: Must pass `compressors` option to all `parquetReadObjects()` calls. Added to `parquet.ts` as default. ~3KB additional dependency.

### D18. Data Worker â€” Parquet I/O + ë³€í™˜ì„ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì™„ì „ ë¶„ë¦¬

- **ë¬¸ì œ**: í”„ë ˆì„ ì „í™˜ì— ~4.5ì´ˆ ì†Œìš”. ì›ì¸ì€ BROTLI í•´ì œ + Parquet ì»¬ëŸ¼ ë””ì½”ë”©ì´ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ UI í”„ë ˆì„ ë“œë ìœ ë°œ.
- **ë‹¨ìˆœ í”„ë¦¬í˜ì¹­ì´ ì•ˆ ë˜ëŠ” ì´ìœ **: í”„ë¦¬í˜ì¹­ì€ ë™ì¼ ì‘ì—…ì„ ë¯¸ë¦¬ ì‹¤í–‰í•  ë¿, BROTLI í•´ì œ ìì²´ê°€ ë©”ì¸ ìŠ¤ë ˆë“œ CPUë¥¼ ì ìœ . 3í”„ë ˆì„ í”„ë¦¬í˜ì¹­ ì‹œ ë¸”ë¡œí‚¹ì´ 3ë°°ë¡œ ì¦ê°€.
- **í•´ê²°**: `dataWorker.ts` â€” ì „ì²´ íŒŒì´í”„ë¼ì¸(fetch â†’ BROTLI í•´ì œ â†’ Parquet ë””ì½”ë”© â†’ range image â†’ xyz ë³€í™˜)ì„ Web Workerì—ì„œ ì‹¤í–‰. ë©”ì¸ ìŠ¤ë ˆë“œëŠ” ìµœì¢… `Float32Array`ë§Œ `transfer`ë¡œ ìˆ˜ì‹  (zero-copy).
- **ì•„í‚¤í…ì²˜ â€” ëª¨ë“ˆ ì±…ì„ ë¶„ë¦¬ ìœ ì§€**:
  ```
  dataWorker.ts (ì–‡ì€ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜, ~130ì¤„)
    â”œâ”€â”€ import { readFrameData } from parquet.ts      â† Parquet I/O ì±…ì„ (ë³€ê²½ ì—†ìŒ)
    â”œâ”€â”€ import { convertAllSensors } from rangeImage.ts â† ë³€í™˜ ì±…ì„ (ë³€ê²½ ì—†ìŒ)
    â””â”€â”€ postMessage(Float32Array, [buffer])             â† zero-copy transfer
  ```
  WorkerëŠ” ê¸°ì¡´ ëª¨ë“ˆì„ importí•´ì„œ í˜¸ì¶œë§Œ í•¨. ê° ëª¨ë“ˆì˜ ë‹¨ì¼ ì±…ì„ ì›ì¹™ ìœ ì§€. Viteì˜ `new Worker(new URL(...))` ë¬¸ë²•ì´ importë¥¼ ìë™ ë²ˆë“¤ë§.
- **í†µì‹  íŒ¨í„´**: Promise-based request/response. `requestId`ë¡œ ë™ì‹œ ë‹¤ë°œ í”„ë¦¬í˜ì¹˜ ìš”ì²­ êµ¬ë¶„.
  ```
  Main Thread                          Data Worker
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  init(lidarUrl, calibrations) â”€â”€â†’   openParquetFile + buildFrameIndex
                               â†â”€â”€   { type: 'ready' }
  loadFrame(requestId, ts)     â”€â”€â†’   readFrameData â†’ convertAllSensors
                               â†â”€â”€   { type: 'frameReady', positions: Float32Array } (transfer)
  loadFrame(requestId+1, ts)   â”€â”€â†’   (í”„ë¦¬í˜ì¹˜ â€” ë™ì‹œ ì²˜ë¦¬)
  loadFrame(requestId+2, ts)   â”€â”€â†’
  ```
- **í”„ë¦¬í˜ì¹­**: í˜„ì¬ í”„ë ˆì„ ë¡œë“œ ì™„ë£Œ í›„ ë‹¤ìŒ 3í”„ë ˆì„ì„ Workerì— ìš”ì²­. Workerê°€ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ë©”ì¸ ìŠ¤ë ˆë“œ ë¸”ë¡œí‚¹ ì œë¡œ. ìˆœì°¨ íƒìƒ‰ ì‹œ ìºì‹œ íˆíŠ¸ë¡œ ì¦‰ì‹œ ì „í™˜.
- **YouTube-style buffer bar**: `cachedFrames: number[]` ìƒíƒœë¡œ ìºì‹œëœ í”„ë ˆì„ ì¸ë±ìŠ¤ë¥¼ Reactì— ë…¸ì¶œ. Timelineì—ì„œ ì—°ì† êµ¬ê°„ì„ ê³„ì‚°í•˜ì—¬ ë°˜íˆ¬ëª… ë°”ë¡œ í‘œì‹œ. ìœ ì €ê°€ í”„ë¦¬í˜ì¹˜ ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥.
- **ì„±ëŠ¥ ì˜í–¥**:
  - ì´ì „: í”„ë ˆì„ ì „í™˜ ì‹œ ë©”ì¸ ìŠ¤ë ˆë“œ ~4.5ì´ˆ ë¸”ë¡œí‚¹ â†’ UI ë©ˆì¶¤
  - ì´í›„: ë©”ì¸ ìŠ¤ë ˆë“œ ë¸”ë¡œí‚¹ 0ms (postMessage ìˆ˜ì‹  + ìºì‹œ ì €ì¥ë§Œ). Workerì—ì„œ ~4.5ì´ˆ ì²˜ë¦¬ë˜ì§€ë§Œ UIëŠ” 60fps ìœ ì§€.
  - í”„ë¦¬í˜ì¹˜ ì ì¤‘ ì‹œ: í”„ë ˆì„ ì „í™˜ 0ms (ìºì‹œì—ì„œ ì¦‰ì‹œ ë¡œë“œ)
- **ë©´ì ‘ í¬ì¸íŠ¸**: "162MB LiDAR Parquetì˜ BROTLI í•´ì œê°€ ë©”ì¸ ìŠ¤ë ˆë“œë¥¼ 4.5ì´ˆ ë¸”ë¡œí‚¹í•˜ëŠ” ë¬¸ì œë¥¼ ë°œê²¬í•˜ê³ , Data Worker + transfer íŒ¨í„´ìœ¼ë¡œ ë©”ì¸ ìŠ¤ë ˆë“œ ë¸”ë¡œí‚¹ì„ ì œë¡œë¡œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ parquet.tsì™€ rangeImage.ts ëª¨ë“ˆì€ ë³€ê²½ ì—†ì´ Workerì—ì„œ importí•˜ì—¬ ì¬ì‚¬ìš© â€” ë‹¨ì¼ ì±…ì„ ì›ì¹™ì„ ìœ ì§€í•˜ë©´ì„œ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ë§Œ ì´ë™í–ˆìŠµë‹ˆë‹¤."

### D17. CPU ë³€í™˜ ì„±ëŠ¥ regression guard â€” `lastConvertMs < 50ms`
- **ëª©ì **: `convertAllSensors()` (range image â†’ xyz) ì•Œê³ ë¦¬ì¦˜ ë³€ê²½ ì‹œ ì„±ëŠ¥ í‡´í–‰ì„ ë¡œì»¬ í…ŒìŠ¤íŠ¸ì—ì„œ ìë™ ê°ì§€.
- **ì¸¡ì • ê¸°ì¤€**: M2 MacBook Airì—ì„œ 5 ì„¼ì„œ ~168K í¬ì¸íŠ¸ ê¸°ì¤€ ~5ms. Thresholdì€ 10ë°° ë§ˆì§„ì¸ 50ms.
- **ì ìš© ìœ„ì¹˜**: `useSceneStore.test.ts`ì˜ frame ë¡œë”© í…ŒìŠ¤íŠ¸ (`nextFrame`, `seekFrame`, `first frame timing`)ì— `expect(lastConvertMs).toBeLessThan(50)` assertion.
- **ê°ì§€ ë¶ˆê°€ëŠ¥í•œ ê²ƒ**: Parquet I/O ì„±ëŠ¥ (8ì´ˆ ì¤‘ 5msë§Œ ë³€í™˜ì´ë¯€ë¡œ I/O ë…¸ì´ì¦ˆì— ë¬»í˜). GPU ë³€í™˜ (Dawn ì†Œí”„íŠ¸ì›¨ì–´ ë Œë”ëŸ¬ëŠ” ë¹„í˜„ì‹¤ì , ë¸Œë¼ìš°ì € í”„ë¡œíŒŒì¼ë§ í•„ìš”).
- **ë³´ì™„**: `rangeImageBenchmark.test.ts`ì—ì„œ ìˆœìˆ˜ ë³€í™˜ë§Œ 5íšŒ ë°˜ë³µ í‰ê·  ì¸¡ì • ê°€ëŠ¥ (ê¸°ë³¸ vitest runì—ì„œ ì œì™¸, ìˆ˜ë™ ì‹¤í–‰).

### D16. State management â†’ Zustand
- **Alternatives considered**: (A) Zustand, (B) `useSyncExternalStore` + TS class, (C) Context + `useReducer`
- **Reasoning**:
  - **(C) Context íƒˆë½**: ìƒíƒœ í•˜ë‚˜ ë°”ë€Œë©´ íŠ¸ë¦¬ ì „ì²´ ë¦¬ë Œë”. 168K í¬ì¸íŠ¸ ë§¤ í”„ë ˆì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ì´ í”„ë¡œì íŠ¸ì—ì„œ ì¹˜ëª…ì .
  - **(B) `useSyncExternalStore` ê²€í† **: ì˜ì¡´ì„± ì œë¡œ, ë©´ì ‘ ì„íŒ©íŠ¸. ê·¸ëŸ¬ë‚˜ subscribe/emit/getSnapshot ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ê°€ ì»¤ì§. Zustand ë‚´ë¶€ë„ `useSyncExternalStore` ê¸°ë°˜ì´ë¯€ë¡œ ì„±ëŠ¥ ë™ì¼í•˜ë©´ì„œ ì½”ë“œê°€ í›¨ì”¬ ì‹¬í”Œ.
  - **(A) Zustand ì±„íƒ**: selector ê¸°ë°˜ ìŠ¬ë¼ì´ìŠ¤ êµ¬ë…ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë¦¬ë Œë” ì—†ìŒ. React ë°–ì—ì„œ `getState()` ì ‘ê·¼ ê°€ëŠ¥ (Worker ê²°ê³¼ ë°˜ì˜ ë“±). ~1KB. ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ìµœì†Œ. ë‚˜ì¤‘ì— `devtools`/`persist` middleware í•„ìš”í•˜ë©´ í•œ ì¤„ ì¶”ê°€ë¡œ ë.
- **êµ¬í˜„**: `useSceneStore` â€” Zustand storeì— state + actions í†µí•©. ë‚´ë¶€ ë°ì´í„°(Parquet files, frame indices, cache)ëŠ” ëª¨ë“ˆ ìŠ¤ì½”í”„ì— ë¶„ë¦¬í•˜ì—¬ React ë¦¬ë Œë”ì—ì„œ ì œì™¸.

## 11. Next Actions

1. âœ… Project scaffolding (Vite + React + TS + R3F)
2. âœ… Waymo Dataset v2.0 download (sample segment)
3. âœ… Parquet schema analysis
4. âœ… Parquet loading infrastructure (hyparquet + merge + tests â€” 27 passing)
5. âœ… Range image â†’ xyz pure math (rangeImage.ts + 14 tests passing against real Waymo data)
6. âœ… CPU Web Worker + WebGPU compute shader (lidarWorker.ts, rangeImageGpu.ts, GPU vs CPU 3 tests passing)
7. â¬œ Phase 1 MVP implementation
7. â¬œ Street Gaussians training
8. â¬œ Deploy + LinkedIn post + Amy DM
